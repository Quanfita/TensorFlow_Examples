{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#先导入常用库和MNIST数据\n",
    "import numpy as np\n",
    "import sklearn.preprocessing as prep\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#自编码器中会使用到一种参数初始化方法Xavier initialization\n",
    "#通过tf.random_uniform创建了一个均匀分布\n",
    "#fan_in是输入节点的数量，fan_out是输出节点的数量\n",
    "def xavier_init(fan_in,fan_out,constant = 1):\n",
    "    low = -constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "    high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in,fan_out),\n",
    "                            minval = low, maxval = high,\n",
    "                            dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义一个去噪自编码的class，方便以后使用，包含一个构建函数__init__(),还有一些常用的成员函数\n",
    "class AdditiveGaussianNoiseAutoencoder(object):\n",
    "    #__init__函数包含这样几个输入：\n",
    "    #n_input（输入变量数）、n_hidden（隐含层节点数）、n_transfer_function（隐含层激活函数，默认为softplus）、\n",
    "    #optimizer（优化器，默认为Adam）、scale（高斯噪声系数，默认为0.1）\n",
    "    #其中，class内的scale参数做成一个placeholder，初始化采用了_initialize_weights函数\n",
    "    def __init__(self,n_input,n_hidden,transfer_function = tf.nn.softplus,\n",
    "                optimizer = tf.train.AdamOptimizer(),scale = 0.1):\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.transfer = transfer_function\n",
    "        self.scale = tf.placeholder(tf.float32)\n",
    "        self.training_scale = scale\n",
    "        network_weights = self._initialize_weights()\n",
    "        self.weights = network_weights\n",
    "        \n",
    "        #接下来定义网络结构，我们为输入x创建一个维度为n_input的placeholder\n",
    "        #然后建立一个能提取特征的隐含层，我们先将输入的x加上噪声，即self.x+scale*tf.random_normal((n_input,))\n",
    "        #然后用tf.matmul将加了噪声的输入与隐含层的权重w1相乘，并使用tf.add加上隐含层的偏置b1\n",
    "        #最后使用self.transfer对结果进行激活函数处理\n",
    "        #经过隐含层后，我们需要在输出层进行数据复原、重建操作（即建立reconstruction层）\n",
    "        #这里就不需要激活函数了，直接将隐含层的输出self.hidden乘上输出层的权重w2，再加上输出层的偏置b2即可\n",
    "        self.x = tf.placeholder(tf.float32,[None,self.n_input])\n",
    "        self.hidden = self.transfer(tf.add(tf.matmul(self.x+scale*tf.random_normal((n_input,)),\n",
    "                                                    self.weights['w1']),self.weights['b1']))\n",
    "        self.reconstruction = tf.add(tf.matmul(self.hidden,self.weights['w2']),self.weights['b2'])\n",
    "        \n",
    "        #接下来定义自编码器的损失函数，这里直接使用平方误差作为cost\n",
    "        #即用tf.substract计算输出（self.reconstruction）与输入（self.x）只差\n",
    "        #在使用tf.pow求差的平方，最后使用tf.reduce_sum求和即可得到平方误差\n",
    "        #再定义训练操作为优化器self.optimizer对损失self.cost\n",
    "        #最后创建Session，并初始化自编码器的全部模型参数\n",
    "        self.cost = 0.5*tf.reduce_sum(tf.pow(tf.subtract(self.reconstruction,\n",
    "                                                        self.x),2.0))\n",
    "        self.optimizer = optimizer.minimize(self.cost)\n",
    "        \n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "    #创建一个名为all_weights的字典dict，然后将w1、b1、w2、b2全部存入其中，最后返回all_weights\n",
    "    #其中w1需要用前面定义的xavier_init函数初始化，我们直接传入输入节点数和隐含节点数\n",
    "    #然后Xavier即可返回一个比较适合于softplus等激活函数的权重初始分布\n",
    "    #而偏置b1只需要使用tf.zeros全部置为0即可\n",
    "    #对于输出层self.reconstruction，因为没有使用激活函数，这里将w2、b2全部初始化为0即可\n",
    "    def _initialize_weights(self):\n",
    "        all_weights = dict()\n",
    "        all_weights['w1'] = tf.Variable(xavier_init(self.n_input,self.n_hidden))\n",
    "        all_weights['b1'] = tf.Variable(tf.zeros([self.n_hidden],dtype = tf.float32))\n",
    "        all_weights['w2'] = tf.Variable(tf.zeros([self.n_hidden,\n",
    "                                                 self.n_input],dtype = tf.float32))\n",
    "        all_weights['b2'] = tf.Variable(tf.zeros([self.n_input],dtype = tf.float32))\n",
    "        \n",
    "        return all_weights\n",
    "    \n",
    "    #定义计算损失cost及执行一步训练的函数\n",
    "    #函数里只需让Session执行两个计算图的节点，分明是损失函数cost和训练过程optimizer\n",
    "    #输入的feed_dict包括输入数据x，以及噪声的系数scale\n",
    "    def partial_fit(self,X):\n",
    "        cost,opt = self.sess.run((self.cost,self.optimizer),\n",
    "                                feed_dict = {self.x:X,self.scale:self.training_scale})\n",
    "        return cost\n",
    "    \n",
    "    #只求损失cost的函数，评测时会用到\n",
    "    def calc_total_cost(self,X):\n",
    "        return self.sess.run(self.cost,feed_dict = {self.x:X,self.scale:self.training_scale})\n",
    "    \n",
    "    #返回自编码器隐含层的输出结果\n",
    "    #目的是提供一个接口来获取抽象后的特征\n",
    "    #自编码器的隐含层的最主要功能就是学习出数据中的高阶特征\n",
    "    def transform(self,X):\n",
    "        return self.sess.run(self.hidden,feed_dict = {self.x:X,self.scale:eslf.training_scale})\n",
    "    \n",
    "    #将隐含层的输出结果作为输入，通过之后重建层将提取到的高阶特征复原为原始数据\n",
    "    #这个接口和前面的transform正好将整个自编码器拆分为两部分，这里的generate接口是后半部分\n",
    "    def generate(self,hidden = None):\n",
    "        if hidden is None:\n",
    "            hidden = np.random.normal(size = self.weights[\"b1\"])\n",
    "        return self.sess.run(self.reconstruction,\n",
    "                            feed_dict = {self.hidden:hidden})\n",
    "    \n",
    "    #reconstruct函数，它整体运行一遍复原过程，包括提取高阶特征和通过高阶特征复原数据\n",
    "    #输入数据是原数据，输出数据是复原后的数据\n",
    "    def reconstruct(self,X):\n",
    "        return self.sess.run(self.reconstruction,feed_dict = {self.x:X,\n",
    "                                                              self.scale:self.training_scale})\n",
    "    \n",
    "    #获取隐含层的权重w1\n",
    "    def getWeights(self):\n",
    "        return self.sess.run(self.weights['w1'])\n",
    "    \n",
    "    #获取隐含层的偏置系数b1\n",
    "    def getBiases(self):\n",
    "        return self.sess.run(self.weights['b1'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#读取示例数据\n",
    "mnist = input_data.read_data_sets('MNIST_data',one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#先定义一个对训练、测试数据进行标准化处理的函数\n",
    "def standard_scale(X_train,X_test):\n",
    "    preprocessor = prep.StandardScaler().fit(X_train)\n",
    "    X_train = preprocessor.transform(X_train)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "    return X_train,X_test\n",
    "\n",
    "#再定义一个获取随机block数据的函数：取一个从0到len(data)-batch_size之间的随机整数\n",
    "#再以这个随机数作为block的起始位置，然后顺序取到一个batch size的数据\n",
    "def get_random_block_from_data(data,batch_size):\n",
    "    start_index = np.random.randint(0,len(data)-batch_size)\n",
    "    return data[start_index:(start_index+batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 18554.658830682\n",
      "Epoch: 0002 cost= 11764.569028409\n",
      "Epoch: 0003 cost= 11036.382665341\n",
      "Epoch: 0004 cost= 10020.621241477\n",
      "Epoch: 0005 cost= 9791.331619318\n",
      "Epoch: 0006 cost= 9311.560923295\n",
      "Epoch: 0007 cost= 9445.958394318\n",
      "Epoch: 0008 cost= 9330.785458523\n",
      "Epoch: 0009 cost= 8845.278918182\n",
      "Epoch: 0010 cost= 7938.236021591\n",
      "Epoch: 0011 cost= 9132.059076136\n",
      "Epoch: 0012 cost= 7817.297751136\n",
      "Epoch: 0013 cost= 8617.920363636\n",
      "Epoch: 0014 cost= 8801.779474432\n",
      "Epoch: 0015 cost= 7779.604441477\n",
      "Epoch: 0016 cost= 8186.543681250\n",
      "Epoch: 0017 cost= 8073.063432955\n",
      "Epoch: 0018 cost= 9403.774734091\n",
      "Epoch: 0019 cost= 8655.932789773\n",
      "Epoch: 0020 cost= 7965.612747159\n",
      "Total cost:670790.0\n"
     ]
    }
   ],
   "source": [
    "#使用standard_scale函数对训练集、测试集进行标准化交换\n",
    "X_train,X_test = standard_scale(mnist.train.images,mnist.test.images)\n",
    "#定义几个常用参数，总训练样本数，最大训练的轮数（epoch）设为20，batch_size设为128\n",
    "#并设置每隔一轮（epoch）就显示一次损失cost\n",
    "n_samples = int(mnist.train.num_examples)\n",
    "training_epochs = 20\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "\n",
    "#创建一个AGN自编码器的实例\n",
    "autoencoder = AdditiveGaussianNoiseAutoencoder(n_input = 784,\n",
    "                                              n_hidden = 200,\n",
    "                                              transfer_function = tf.nn.softplus,\n",
    "                                              optimizer = tf.train.AdamOptimizer(learning_rate = 0.001),\n",
    "                                              scale = 0.01)\n",
    "\n",
    "#下面开始训练过程\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(n_samples/batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs = get_random_block_from_data(X_train,batch_size)\n",
    "        \n",
    "        cost = autoencoder.partial_fit(batch_xs)\n",
    "        avg_cost += cost / n_samples * batch_size\n",
    "        \n",
    "    if epoch % display_step == 0:\n",
    "        print(\"Epoch:\",'%04d' % (epoch+1),\"cost=\",\"{:.9f}\".format(avg_cost))\n",
    "\n",
    "#对训练完的模型进行性能测试\n",
    "print(\"Total cost:\"+str(autoencoder.calc_total_cost(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
