{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class gameOb():\n",
    "    def __init__(self,coordinates,size,intensity,channel,reward,name):\n",
    "        self.x = coordinates[0]\n",
    "        self.y = coordinates[1]\n",
    "        self.size = size\n",
    "        self.intensity = intensity\n",
    "        self.channel = channel\n",
    "        self.reward = reward\n",
    "        self.name = name\n",
    "class gameEnv():\n",
    "    def __init__(self,size):\n",
    "        self.sizeX = size\n",
    "        self.sizeY = size\n",
    "        self.actions = 4\n",
    "        self.objects = []\n",
    "        a = self.reset()\n",
    "        plt.imshow(a,interpolation=\"nearest\")\n",
    "    def reset(self):\n",
    "        self.objects = []\n",
    "        hero = gameOb(self.newPosition(),1,1,2,None,'hero')\n",
    "        self.objects.append(hero)\n",
    "        goal = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(goal)\n",
    "        hole = gameOb(self.newPosition(),1,1,0,-1,'fire')\n",
    "        self.objects.append(hole)\n",
    "        goal2 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(goal2)\n",
    "        hole2 = gameOb(self.newPosition(),1,1,0,-1,'fire')\n",
    "        self.objects.append(hole2)\n",
    "        goal3 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(goal3)\n",
    "        goal4 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(goal4)\n",
    "        state = self.renderEnv()\n",
    "        self.state = state\n",
    "        return state\n",
    "    def moveChar(self,direction):\n",
    "        hero = self.objects[0]\n",
    "        heroX = hero.x\n",
    "        heroY = hero.y\n",
    "        if direction == 0 and hero.y >=1:\n",
    "            hero.y -= 1\n",
    "        if direction == 1 and hero.y <= self.sizeY-2:\n",
    "            hero.y += 1\n",
    "        if direction == 2 and hero.x >=1:\n",
    "            hero.x -= 1\n",
    "        if direction == 3 and hero.x <= self.sizeX-2:\n",
    "            hero.x += 1\n",
    "        self.objects[0] = hero\n",
    "    def newPosition(self):\n",
    "        iterables = [range(self.sizeX),range(self.sizeY)]\n",
    "        points = []\n",
    "        for t in itertools.product(*iterables):\n",
    "            points.append(t)\n",
    "        currentPositions = []\n",
    "        for objectA in self.objects:\n",
    "            if(objectA.x,objectA.y) not in currentPositions:\n",
    "                currentPositions.append((objectA.x,objectA.y))\n",
    "        for pos in currentPositions:\n",
    "            points.remove(pos)\n",
    "        location = np.random.choice(range(len(points)),replace=False)\n",
    "        return points[location]\n",
    "    def checkGoal(self):\n",
    "        others = []\n",
    "        for obj in self.objects:\n",
    "            if obj.name == 'hero':\n",
    "                hero = obj\n",
    "            else:\n",
    "                others.append(obj)\n",
    "        for other in others:\n",
    "            if hero.x == other.x and hero.y == other.y:\n",
    "                self.objects.remove(other)\n",
    "                if other.reward == 1:\n",
    "                    self.objects.append(gameOb(self.newPosition(),1,1,1,1,'goal'))\n",
    "                else:\n",
    "                    self.objects.append(gameOb(self.newPosition(),1,1,0,-1,'fire'))\n",
    "                return other.reward,False\n",
    "        return 0.0,False\n",
    "    def renderEnv(self):\n",
    "        a = np.ones([self.sizeY+2,self.sizeX+2,3])\n",
    "        a[1:-1,1:-1,:] = 0\n",
    "        hero = None\n",
    "        for item in self.objects:\n",
    "            a[item.y+1:item.y+item.size+1,item.x+1:item.x+item.size+1,item.channel] = item.intensity\n",
    "        b = scipy.misc.imresize(a[:,:,0],[84,84,1],interp='nearest')\n",
    "        c = scipy.misc.imresize(a[:,:,1],[84,84,1],interp='nearest')\n",
    "        d = scipy.misc.imresize(a[:,:,2],[84,84,2],interp='nearest')\n",
    "        a = np.stack([b,c,d],axis=2)\n",
    "        return a\n",
    "    def step(self,action):\n",
    "        self.moveChar(action)\n",
    "        reward,done = self.checkGoal()\n",
    "        state = self.renderEnv()\n",
    "        return state,reward,done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23691f03a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gameEnv(size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size):\n",
    "        self.scalarInput = tf.placeholder(shape=[None,21168],dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,84,84,3])\n",
    "        self.conv1 = tf.contrib.layers.convolution2d(inputs=self.imageIn,num_outputs=32,\n",
    "                                                    kernel_size=[8,8],stride=[4,4],\n",
    "                                                    padding='VALID',biases_initializer=None)\n",
    "        self.conv2 = tf.contrib.layers.convolution2d(inputs=self.conv1,num_outputs=64,\n",
    "                                                    kernel_size=[4,4],stride=[2,2],\n",
    "                                                    padding='VALID',biases_initializer=None)\n",
    "        self.conv3 = tf.contrib.layers.convolution2d(inputs=self.conv2,num_outputs=64,\n",
    "                                                    kernel_size=[3,3],stride=[1,1],\n",
    "                                                    padding='VALID',biases_initializer=None)\n",
    "        self.conv4 = tf.contrib.layers.convolution2d(inputs=self.conv3,num_outputs=512,\n",
    "                                                    kernel_size=[7,7],stride=[1,1],\n",
    "                                                    padding='VALID',biases_initializer=None)\n",
    "        self.streamAC,self.streamVC = tf.split(self.conv4,2,3)\n",
    "        self.streamA = tf.contrib.layers.flatten(self.streamAC)\n",
    "        self.streamV = tf.contrib.layers.flatten(self.streamVC)\n",
    "        self.AW = tf.Variable(tf.random_normal([h_size//2,env.actions]))\n",
    "        self.VW = tf.Variable(tf.random_normal([h_size//2,1]))\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,\n",
    "                                                                          reduction_indices=1,keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,env.actions,dtype=tf.float32)\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout,self.actions_onehot),reduction_indices=1)\n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self,buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer)+len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))- \\\n",
    "                        self.buffer_size]=[]\n",
    "        self.buffer.extend(experience)\n",
    "        \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processState(states):\n",
    "    return np.reshape(states,[21168])\n",
    "\n",
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau)+((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "update_freq = 4\n",
    "y = .99\n",
    "startE = 1\n",
    "endE = 0.1\n",
    "anneling_steps = 10000.\n",
    "num_episodes = 10000\n",
    "pre_train_steps = 10000\n",
    "max_epLength = 50\n",
    "load_model = False\n",
    "path = \"./dqn\"\n",
    "h_size = 512\n",
    "tau = 0.001\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "e = startE\n",
    "stepDrop = (startE-endE)/anneling_steps\n",
    "\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 25 ,average reward of last 25 episode 2.28\n",
      "episode 50 ,average reward of last 25 episode 2.0\n",
      "episode 75 ,average reward of last 25 episode 0.92\n",
      "episode 100 ,average reward of last 25 episode 1.8\n",
      "episode 125 ,average reward of last 25 episode 2.24\n",
      "episode 150 ,average reward of last 25 episode 1.64\n",
      "episode 175 ,average reward of last 25 episode 1.96\n",
      "episode 200 ,average reward of last 25 episode 1.64\n",
      "episode 225 ,average reward of last 25 episode 1.56\n",
      "episode 250 ,average reward of last 25 episode 1.44\n",
      "episode 275 ,average reward of last 25 episode 2.56\n",
      "episode 300 ,average reward of last 25 episode 1.52\n",
      "episode 325 ,average reward of last 25 episode 2.24\n",
      "episode 350 ,average reward of last 25 episode 1.92\n",
      "episode 375 ,average reward of last 25 episode 1.16\n",
      "episode 400 ,average reward of last 25 episode 0.32\n",
      "episode 425 ,average reward of last 25 episode 0.56\n",
      "episode 450 ,average reward of last 25 episode 1.52\n",
      "episode 475 ,average reward of last 25 episode 0.72\n",
      "episode 500 ,average reward of last 25 episode 1.2\n",
      "episode 525 ,average reward of last 25 episode 1.28\n",
      "episode 550 ,average reward of last 25 episode 0.52\n",
      "episode 575 ,average reward of last 25 episode 1.4\n",
      "episode 600 ,average reward of last 25 episode 1.44\n",
      "episode 625 ,average reward of last 25 episode 1.0\n",
      "episode 650 ,average reward of last 25 episode 1.16\n",
      "episode 675 ,average reward of last 25 episode 0.88\n",
      "episode 700 ,average reward of last 25 episode 1.4\n",
      "episode 725 ,average reward of last 25 episode 1.32\n",
      "episode 750 ,average reward of last 25 episode 0.8\n",
      "episode 775 ,average reward of last 25 episode 0.84\n",
      "episode 800 ,average reward of last 25 episode 1.68\n",
      "episode 825 ,average reward of last 25 episode 1.0\n",
      "episode 850 ,average reward of last 25 episode 1.32\n",
      "episode 875 ,average reward of last 25 episode 0.96\n",
      "episode 900 ,average reward of last 25 episode 1.56\n",
      "episode 925 ,average reward of last 25 episode 1.08\n",
      "episode 950 ,average reward of last 25 episode 1.16\n",
      "episode 975 ,average reward of last 25 episode 1.28\n",
      "episode 1000 ,average reward of last 25 episode 1.4\n",
      "Saved Model\n",
      "episode 1025 ,average reward of last 25 episode 1.76\n",
      "episode 1050 ,average reward of last 25 episode 1.08\n",
      "episode 1075 ,average reward of last 25 episode 2.04\n",
      "episode 1100 ,average reward of last 25 episode 1.6\n",
      "episode 1125 ,average reward of last 25 episode 1.72\n",
      "episode 1150 ,average reward of last 25 episode 1.56\n",
      "episode 1175 ,average reward of last 25 episode 1.92\n",
      "episode 1200 ,average reward of last 25 episode 2.4\n",
      "episode 1225 ,average reward of last 25 episode 1.52\n",
      "episode 1250 ,average reward of last 25 episode 1.64\n",
      "episode 1275 ,average reward of last 25 episode 1.52\n",
      "episode 1300 ,average reward of last 25 episode 1.64\n",
      "episode 1325 ,average reward of last 25 episode 1.64\n",
      "episode 1350 ,average reward of last 25 episode 2.08\n",
      "episode 1375 ,average reward of last 25 episode 1.36\n",
      "episode 1400 ,average reward of last 25 episode 2.04\n",
      "episode 1425 ,average reward of last 25 episode 2.24\n",
      "episode 1450 ,average reward of last 25 episode 2.12\n",
      "episode 1475 ,average reward of last 25 episode 1.72\n",
      "episode 1500 ,average reward of last 25 episode 1.8\n",
      "episode 1525 ,average reward of last 25 episode 2.64\n",
      "episode 1550 ,average reward of last 25 episode 1.76\n",
      "episode 1575 ,average reward of last 25 episode 3.4\n",
      "episode 1600 ,average reward of last 25 episode 2.8\n",
      "episode 1625 ,average reward of last 25 episode 2.08\n",
      "episode 1650 ,average reward of last 25 episode 2.28\n",
      "episode 1675 ,average reward of last 25 episode 3.56\n",
      "episode 1700 ,average reward of last 25 episode 2.52\n",
      "episode 1725 ,average reward of last 25 episode 4.12\n",
      "episode 1750 ,average reward of last 25 episode 2.6\n",
      "episode 1775 ,average reward of last 25 episode 3.84\n",
      "episode 1800 ,average reward of last 25 episode 2.88\n",
      "episode 1825 ,average reward of last 25 episode 3.48\n",
      "episode 1850 ,average reward of last 25 episode 3.48\n",
      "episode 1875 ,average reward of last 25 episode 4.24\n",
      "episode 1900 ,average reward of last 25 episode 4.48\n",
      "episode 1925 ,average reward of last 25 episode 3.96\n",
      "episode 1950 ,average reward of last 25 episode 3.84\n",
      "episode 1975 ,average reward of last 25 episode 4.16\n",
      "episode 2000 ,average reward of last 25 episode 5.64\n",
      "Saved Model\n",
      "episode 2025 ,average reward of last 25 episode 5.28\n",
      "episode 2050 ,average reward of last 25 episode 5.44\n",
      "episode 2075 ,average reward of last 25 episode 4.52\n",
      "episode 2100 ,average reward of last 25 episode 6.04\n",
      "episode 2125 ,average reward of last 25 episode 5.4\n",
      "episode 2150 ,average reward of last 25 episode 6.56\n",
      "episode 2175 ,average reward of last 25 episode 6.88\n",
      "episode 2200 ,average reward of last 25 episode 8.44\n",
      "episode 2225 ,average reward of last 25 episode 8.6\n",
      "episode 2250 ,average reward of last 25 episode 8.96\n",
      "episode 2275 ,average reward of last 25 episode 7.88\n",
      "episode 2300 ,average reward of last 25 episode 8.08\n",
      "episode 2325 ,average reward of last 25 episode 9.72\n",
      "episode 2350 ,average reward of last 25 episode 9.68\n",
      "episode 2375 ,average reward of last 25 episode 7.16\n",
      "episode 2400 ,average reward of last 25 episode 10.44\n",
      "episode 2425 ,average reward of last 25 episode 11.32\n",
      "episode 2450 ,average reward of last 25 episode 11.6\n",
      "episode 2475 ,average reward of last 25 episode 9.56\n",
      "episode 2500 ,average reward of last 25 episode 14.28\n",
      "episode 2525 ,average reward of last 25 episode 12.28\n",
      "episode 2550 ,average reward of last 25 episode 14.48\n",
      "episode 2575 ,average reward of last 25 episode 12.84\n",
      "episode 2600 ,average reward of last 25 episode 14.44\n",
      "episode 2625 ,average reward of last 25 episode 12.8\n",
      "episode 2650 ,average reward of last 25 episode 13.04\n",
      "episode 2675 ,average reward of last 25 episode 13.6\n",
      "episode 2700 ,average reward of last 25 episode 15.88\n",
      "episode 2725 ,average reward of last 25 episode 14.76\n",
      "episode 2750 ,average reward of last 25 episode 18.44\n",
      "episode 2775 ,average reward of last 25 episode 15.04\n",
      "episode 2800 ,average reward of last 25 episode 16.76\n",
      "episode 2825 ,average reward of last 25 episode 15.64\n",
      "episode 2850 ,average reward of last 25 episode 14.76\n",
      "episode 2875 ,average reward of last 25 episode 17.36\n",
      "episode 2900 ,average reward of last 25 episode 19.04\n",
      "episode 2925 ,average reward of last 25 episode 18.72\n",
      "episode 2950 ,average reward of last 25 episode 18.6\n",
      "episode 2975 ,average reward of last 25 episode 16.92\n",
      "episode 3000 ,average reward of last 25 episode 18.44\n",
      "Saved Model\n",
      "episode 3025 ,average reward of last 25 episode 18.84\n",
      "episode 3050 ,average reward of last 25 episode 18.28\n",
      "episode 3075 ,average reward of last 25 episode 21.6\n",
      "episode 3100 ,average reward of last 25 episode 18.36\n",
      "episode 3125 ,average reward of last 25 episode 19.2\n",
      "episode 3150 ,average reward of last 25 episode 19.04\n",
      "episode 3175 ,average reward of last 25 episode 19.96\n",
      "episode 3200 ,average reward of last 25 episode 19.04\n",
      "episode 3225 ,average reward of last 25 episode 18.28\n",
      "episode 3250 ,average reward of last 25 episode 20.56\n",
      "episode 3275 ,average reward of last 25 episode 18.8\n",
      "episode 3300 ,average reward of last 25 episode 20.96\n",
      "episode 3325 ,average reward of last 25 episode 19.32\n",
      "episode 3350 ,average reward of last 25 episode 20.52\n",
      "episode 3375 ,average reward of last 25 episode 20.28\n",
      "episode 3400 ,average reward of last 25 episode 19.28\n",
      "episode 3425 ,average reward of last 25 episode 19.32\n",
      "episode 3450 ,average reward of last 25 episode 20.64\n",
      "episode 3475 ,average reward of last 25 episode 21.24\n",
      "episode 3500 ,average reward of last 25 episode 21.84\n",
      "episode 3525 ,average reward of last 25 episode 20.52\n",
      "episode 3550 ,average reward of last 25 episode 19.6\n",
      "episode 3575 ,average reward of last 25 episode 20.84\n",
      "episode 3600 ,average reward of last 25 episode 18.76\n",
      "episode 3625 ,average reward of last 25 episode 21.36\n",
      "episode 3650 ,average reward of last 25 episode 19.48\n",
      "episode 3675 ,average reward of last 25 episode 19.84\n",
      "episode 3700 ,average reward of last 25 episode 21.08\n",
      "episode 3725 ,average reward of last 25 episode 18.48\n",
      "episode 3750 ,average reward of last 25 episode 21.36\n",
      "episode 3775 ,average reward of last 25 episode 20.84\n",
      "episode 3800 ,average reward of last 25 episode 22.28\n",
      "episode 3825 ,average reward of last 25 episode 21.64\n",
      "episode 3850 ,average reward of last 25 episode 22.04\n",
      "episode 3875 ,average reward of last 25 episode 20.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3900 ,average reward of last 25 episode 21.2\n",
      "episode 3925 ,average reward of last 25 episode 20.72\n",
      "episode 3950 ,average reward of last 25 episode 22.72\n",
      "episode 3975 ,average reward of last 25 episode 22.56\n",
      "episode 4000 ,average reward of last 25 episode 22.08\n",
      "Saved Model\n",
      "episode 4025 ,average reward of last 25 episode 21.56\n",
      "episode 4050 ,average reward of last 25 episode 21.04\n",
      "episode 4075 ,average reward of last 25 episode 19.88\n",
      "episode 4100 ,average reward of last 25 episode 21.68\n",
      "episode 4125 ,average reward of last 25 episode 20.64\n",
      "episode 4150 ,average reward of last 25 episode 21.48\n",
      "episode 4175 ,average reward of last 25 episode 22.16\n",
      "episode 4200 ,average reward of last 25 episode 22.28\n",
      "episode 4225 ,average reward of last 25 episode 22.76\n",
      "episode 4250 ,average reward of last 25 episode 22.2\n",
      "episode 4275 ,average reward of last 25 episode 22.72\n",
      "episode 4300 ,average reward of last 25 episode 21.48\n",
      "episode 4325 ,average reward of last 25 episode 21.56\n",
      "episode 4350 ,average reward of last 25 episode 21.4\n",
      "episode 4375 ,average reward of last 25 episode 22.4\n",
      "episode 4400 ,average reward of last 25 episode 20.44\n",
      "episode 4425 ,average reward of last 25 episode 21.96\n",
      "episode 4450 ,average reward of last 25 episode 20.52\n",
      "episode 4475 ,average reward of last 25 episode 20.8\n",
      "episode 4500 ,average reward of last 25 episode 22.8\n",
      "episode 4525 ,average reward of last 25 episode 22.52\n",
      "episode 4550 ,average reward of last 25 episode 22.2\n",
      "episode 4575 ,average reward of last 25 episode 22.88\n",
      "episode 4600 ,average reward of last 25 episode 20.6\n",
      "episode 4625 ,average reward of last 25 episode 21.68\n",
      "episode 4650 ,average reward of last 25 episode 22.24\n",
      "episode 4675 ,average reward of last 25 episode 23.56\n",
      "episode 4700 ,average reward of last 25 episode 22.4\n",
      "episode 4725 ,average reward of last 25 episode 21.24\n",
      "episode 4750 ,average reward of last 25 episode 22.76\n",
      "episode 4775 ,average reward of last 25 episode 23.52\n",
      "episode 4800 ,average reward of last 25 episode 23.48\n",
      "episode 4825 ,average reward of last 25 episode 22.68\n",
      "episode 4850 ,average reward of last 25 episode 20.88\n",
      "episode 4875 ,average reward of last 25 episode 23.28\n",
      "episode 4900 ,average reward of last 25 episode 22.04\n",
      "episode 4925 ,average reward of last 25 episode 23.12\n",
      "episode 4950 ,average reward of last 25 episode 21.28\n",
      "episode 4975 ,average reward of last 25 episode 21.96\n",
      "episode 5000 ,average reward of last 25 episode 22.52\n",
      "Saved Model\n",
      "episode 5025 ,average reward of last 25 episode 23.56\n",
      "episode 5050 ,average reward of last 25 episode 23.76\n",
      "episode 5075 ,average reward of last 25 episode 21.8\n",
      "episode 5100 ,average reward of last 25 episode 22.32\n",
      "episode 5125 ,average reward of last 25 episode 21.88\n",
      "episode 5150 ,average reward of last 25 episode 22.28\n",
      "episode 5175 ,average reward of last 25 episode 23.56\n",
      "episode 5200 ,average reward of last 25 episode 21.64\n",
      "episode 5225 ,average reward of last 25 episode 22.64\n",
      "episode 5250 ,average reward of last 25 episode 22.16\n",
      "episode 5275 ,average reward of last 25 episode 22.44\n",
      "episode 5300 ,average reward of last 25 episode 23.24\n",
      "episode 5325 ,average reward of last 25 episode 20.36\n",
      "episode 5350 ,average reward of last 25 episode 23.52\n",
      "episode 5375 ,average reward of last 25 episode 22.72\n",
      "episode 5400 ,average reward of last 25 episode 20.2\n",
      "episode 5425 ,average reward of last 25 episode 21.0\n",
      "episode 5450 ,average reward of last 25 episode 23.52\n",
      "episode 5475 ,average reward of last 25 episode 21.84\n",
      "episode 5500 ,average reward of last 25 episode 21.8\n",
      "episode 5525 ,average reward of last 25 episode 22.2\n",
      "episode 5550 ,average reward of last 25 episode 23.04\n",
      "episode 5575 ,average reward of last 25 episode 22.2\n",
      "episode 5600 ,average reward of last 25 episode 22.04\n",
      "episode 5625 ,average reward of last 25 episode 23.28\n",
      "episode 5650 ,average reward of last 25 episode 23.24\n",
      "episode 5675 ,average reward of last 25 episode 23.4\n",
      "episode 5700 ,average reward of last 25 episode 23.08\n",
      "episode 5725 ,average reward of last 25 episode 22.16\n",
      "episode 5750 ,average reward of last 25 episode 21.84\n",
      "episode 5775 ,average reward of last 25 episode 21.48\n",
      "episode 5800 ,average reward of last 25 episode 22.32\n",
      "episode 5825 ,average reward of last 25 episode 22.8\n",
      "episode 5850 ,average reward of last 25 episode 22.96\n",
      "episode 5875 ,average reward of last 25 episode 21.16\n",
      "episode 5900 ,average reward of last 25 episode 24.8\n",
      "episode 5925 ,average reward of last 25 episode 22.28\n",
      "episode 5950 ,average reward of last 25 episode 23.48\n",
      "episode 5975 ,average reward of last 25 episode 21.52\n",
      "episode 6000 ,average reward of last 25 episode 22.44\n",
      "Saved Model\n",
      "episode 6025 ,average reward of last 25 episode 21.96\n",
      "episode 6050 ,average reward of last 25 episode 22.88\n",
      "episode 6075 ,average reward of last 25 episode 22.92\n",
      "episode 6100 ,average reward of last 25 episode 23.28\n",
      "episode 6125 ,average reward of last 25 episode 21.96\n",
      "episode 6150 ,average reward of last 25 episode 22.64\n",
      "episode 6175 ,average reward of last 25 episode 23.68\n",
      "episode 6200 ,average reward of last 25 episode 23.44\n",
      "episode 6225 ,average reward of last 25 episode 23.28\n",
      "episode 6250 ,average reward of last 25 episode 22.4\n",
      "episode 6275 ,average reward of last 25 episode 22.88\n",
      "episode 6300 ,average reward of last 25 episode 23.88\n",
      "episode 6325 ,average reward of last 25 episode 23.4\n",
      "episode 6350 ,average reward of last 25 episode 21.96\n",
      "episode 6375 ,average reward of last 25 episode 23.84\n",
      "episode 6400 ,average reward of last 25 episode 21.44\n",
      "episode 6425 ,average reward of last 25 episode 21.36\n",
      "episode 6450 ,average reward of last 25 episode 20.96\n",
      "episode 6475 ,average reward of last 25 episode 23.68\n",
      "episode 6500 ,average reward of last 25 episode 21.64\n",
      "episode 6525 ,average reward of last 25 episode 21.8\n",
      "episode 6550 ,average reward of last 25 episode 23.28\n",
      "episode 6575 ,average reward of last 25 episode 22.88\n",
      "episode 6600 ,average reward of last 25 episode 23.28\n",
      "episode 6625 ,average reward of last 25 episode 22.6\n",
      "episode 6650 ,average reward of last 25 episode 23.08\n",
      "episode 6675 ,average reward of last 25 episode 21.52\n",
      "episode 6700 ,average reward of last 25 episode 22.52\n",
      "episode 6725 ,average reward of last 25 episode 22.12\n",
      "episode 6750 ,average reward of last 25 episode 23.2\n",
      "episode 6775 ,average reward of last 25 episode 23.12\n",
      "episode 6800 ,average reward of last 25 episode 21.84\n",
      "episode 6825 ,average reward of last 25 episode 22.72\n",
      "episode 6850 ,average reward of last 25 episode 21.0\n",
      "episode 6875 ,average reward of last 25 episode 23.48\n",
      "episode 6900 ,average reward of last 25 episode 23.32\n",
      "episode 6925 ,average reward of last 25 episode 24.04\n",
      "episode 6950 ,average reward of last 25 episode 21.4\n",
      "episode 6975 ,average reward of last 25 episode 22.56\n",
      "episode 7000 ,average reward of last 25 episode 22.72\n",
      "Saved Model\n",
      "episode 7025 ,average reward of last 25 episode 22.8\n",
      "episode 7050 ,average reward of last 25 episode 23.36\n",
      "episode 7075 ,average reward of last 25 episode 22.2\n",
      "episode 7100 ,average reward of last 25 episode 22.56\n",
      "episode 7125 ,average reward of last 25 episode 22.92\n",
      "episode 7150 ,average reward of last 25 episode 22.68\n",
      "episode 7175 ,average reward of last 25 episode 21.44\n",
      "episode 7200 ,average reward of last 25 episode 21.76\n",
      "episode 7225 ,average reward of last 25 episode 24.12\n",
      "episode 7250 ,average reward of last 25 episode 22.36\n",
      "episode 7275 ,average reward of last 25 episode 23.92\n",
      "episode 7300 ,average reward of last 25 episode 23.4\n",
      "episode 7325 ,average reward of last 25 episode 22.64\n",
      "episode 7350 ,average reward of last 25 episode 23.08\n",
      "episode 7375 ,average reward of last 25 episode 21.32\n",
      "episode 7400 ,average reward of last 25 episode 21.68\n",
      "episode 7425 ,average reward of last 25 episode 22.28\n",
      "episode 7450 ,average reward of last 25 episode 23.44\n",
      "episode 7475 ,average reward of last 25 episode 21.72\n",
      "episode 7500 ,average reward of last 25 episode 23.12\n",
      "episode 7525 ,average reward of last 25 episode 22.6\n",
      "episode 7550 ,average reward of last 25 episode 22.64\n",
      "episode 7575 ,average reward of last 25 episode 23.36\n",
      "episode 7600 ,average reward of last 25 episode 22.76\n",
      "episode 7625 ,average reward of last 25 episode 21.08\n",
      "episode 7650 ,average reward of last 25 episode 22.36\n",
      "episode 7675 ,average reward of last 25 episode 23.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 7700 ,average reward of last 25 episode 22.2\n",
      "episode 7725 ,average reward of last 25 episode 21.84\n",
      "episode 7750 ,average reward of last 25 episode 22.2\n",
      "episode 7775 ,average reward of last 25 episode 23.28\n",
      "episode 7800 ,average reward of last 25 episode 24.0\n",
      "episode 7825 ,average reward of last 25 episode 22.52\n",
      "episode 7850 ,average reward of last 25 episode 23.36\n",
      "episode 7875 ,average reward of last 25 episode 22.04\n",
      "episode 7900 ,average reward of last 25 episode 22.16\n",
      "episode 7925 ,average reward of last 25 episode 21.76\n",
      "episode 7950 ,average reward of last 25 episode 22.76\n",
      "episode 7975 ,average reward of last 25 episode 21.84\n",
      "episode 8000 ,average reward of last 25 episode 23.24\n",
      "Saved Model\n",
      "episode 8025 ,average reward of last 25 episode 23.48\n",
      "episode 8050 ,average reward of last 25 episode 23.24\n",
      "episode 8075 ,average reward of last 25 episode 22.36\n",
      "episode 8100 ,average reward of last 25 episode 21.76\n",
      "episode 8125 ,average reward of last 25 episode 24.0\n",
      "episode 8150 ,average reward of last 25 episode 23.96\n",
      "episode 8175 ,average reward of last 25 episode 23.44\n",
      "episode 8200 ,average reward of last 25 episode 21.48\n",
      "episode 8225 ,average reward of last 25 episode 22.48\n",
      "episode 8250 ,average reward of last 25 episode 23.24\n",
      "episode 8275 ,average reward of last 25 episode 23.92\n",
      "episode 8300 ,average reward of last 25 episode 22.64\n",
      "episode 8325 ,average reward of last 25 episode 24.56\n",
      "episode 8350 ,average reward of last 25 episode 22.6\n",
      "episode 8375 ,average reward of last 25 episode 23.12\n",
      "episode 8400 ,average reward of last 25 episode 23.24\n",
      "episode 8425 ,average reward of last 25 episode 20.96\n",
      "episode 8450 ,average reward of last 25 episode 22.28\n",
      "episode 8475 ,average reward of last 25 episode 23.44\n",
      "episode 8500 ,average reward of last 25 episode 23.12\n",
      "episode 8525 ,average reward of last 25 episode 23.04\n",
      "episode 8550 ,average reward of last 25 episode 21.96\n",
      "episode 8575 ,average reward of last 25 episode 22.32\n",
      "episode 8600 ,average reward of last 25 episode 23.16\n",
      "episode 8625 ,average reward of last 25 episode 22.64\n",
      "episode 8650 ,average reward of last 25 episode 22.68\n",
      "episode 8675 ,average reward of last 25 episode 21.0\n",
      "episode 8700 ,average reward of last 25 episode 22.76\n",
      "episode 8725 ,average reward of last 25 episode 22.68\n",
      "episode 8750 ,average reward of last 25 episode 21.6\n",
      "episode 8775 ,average reward of last 25 episode 22.4\n",
      "episode 8800 ,average reward of last 25 episode 22.76\n",
      "episode 8825 ,average reward of last 25 episode 21.16\n",
      "episode 8850 ,average reward of last 25 episode 22.68\n",
      "episode 8875 ,average reward of last 25 episode 22.04\n",
      "episode 8900 ,average reward of last 25 episode 22.56\n",
      "episode 8925 ,average reward of last 25 episode 23.6\n",
      "episode 8950 ,average reward of last 25 episode 21.36\n",
      "episode 8975 ,average reward of last 25 episode 22.76\n",
      "episode 9000 ,average reward of last 25 episode 23.64\n",
      "Saved Model\n",
      "episode 9025 ,average reward of last 25 episode 22.72\n",
      "episode 9050 ,average reward of last 25 episode 24.32\n",
      "episode 9075 ,average reward of last 25 episode 22.68\n",
      "episode 9100 ,average reward of last 25 episode 22.56\n",
      "episode 9125 ,average reward of last 25 episode 23.44\n",
      "episode 9150 ,average reward of last 25 episode 23.16\n",
      "episode 9175 ,average reward of last 25 episode 22.96\n",
      "episode 9200 ,average reward of last 25 episode 22.4\n",
      "episode 9225 ,average reward of last 25 episode 23.0\n",
      "episode 9250 ,average reward of last 25 episode 23.08\n",
      "episode 9275 ,average reward of last 25 episode 22.44\n",
      "episode 9300 ,average reward of last 25 episode 22.92\n",
      "episode 9325 ,average reward of last 25 episode 23.44\n",
      "episode 9350 ,average reward of last 25 episode 22.68\n",
      "episode 9375 ,average reward of last 25 episode 23.04\n",
      "episode 9400 ,average reward of last 25 episode 23.56\n",
      "episode 9425 ,average reward of last 25 episode 22.6\n",
      "episode 9450 ,average reward of last 25 episode 22.56\n",
      "episode 9475 ,average reward of last 25 episode 23.24\n",
      "episode 9500 ,average reward of last 25 episode 22.96\n",
      "episode 9525 ,average reward of last 25 episode 22.44\n",
      "episode 9550 ,average reward of last 25 episode 23.84\n",
      "episode 9575 ,average reward of last 25 episode 22.48\n",
      "episode 9600 ,average reward of last 25 episode 22.32\n",
      "episode 9625 ,average reward of last 25 episode 23.32\n",
      "episode 9650 ,average reward of last 25 episode 22.8\n",
      "episode 9675 ,average reward of last 25 episode 22.8\n",
      "episode 9700 ,average reward of last 25 episode 23.04\n",
      "episode 9725 ,average reward of last 25 episode 23.24\n",
      "episode 9750 ,average reward of last 25 episode 23.04\n",
      "episode 9775 ,average reward of last 25 episode 23.16\n",
      "episode 9800 ,average reward of last 25 episode 23.0\n",
      "episode 9825 ,average reward of last 25 episode 22.68\n",
      "episode 9850 ,average reward of last 25 episode 24.52\n",
      "episode 9875 ,average reward of last 25 episode 20.96\n",
      "episode 9900 ,average reward of last 25 episode 22.6\n",
      "episode 9925 ,average reward of last 25 episode 21.72\n",
      "episode 9950 ,average reward of last 25 episode 22.92\n",
      "episode 9975 ,average reward of last 25 episode 22.56\n",
      "episode 10000 ,average reward of last 25 episode 23.76\n",
      "Saved Model\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Session' object has no attribute 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e119f50f81f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/model-'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.cptk'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Saved Model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/model-'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.cptk'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Session' object has no attribute 'path'"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    sess.run(init)\n",
    "    updateTarget(targetOps,sess)\n",
    "    for i in range(num_episodes+1):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        s = env.reset()\n",
    "        s = processState(s)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        while j < max_epLength:\n",
    "            j+=1\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = np.random.randint(0,4)\n",
    "            else:\n",
    "                a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[0]\n",
    "            s1,r,d = env.step(a)\n",
    "            s1 = processState(s1)\n",
    "            total_steps += 1\n",
    "            episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5]))\n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    trainBatch = myBuffer.sample(batch_size)\n",
    "                    A = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    Q = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    doubleQ = Q[range(batch_size),A]\n",
    "                    targetQ = trainBatch[:,2] + y*doubleQ\n",
    "                    _ = sess.run(mainQN.updateModel,feed_dict={\n",
    "                        mainQN.scalarInput:np.vstack(trainBatch[:,0]),\n",
    "                        mainQN.targetQ:targetQ,\n",
    "                        mainQN.actions:trainBatch[:,1]\n",
    "                    })\n",
    "                    updateTarget(targetOps,sess)\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            \n",
    "            if d == True:\n",
    "                break\n",
    "        \n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        rList.append(rAll)\n",
    "        if i>0 and i%25 == 0:\n",
    "            print('episode',i,',average reward of last 25 episode',np.mean(rList[-25:]))\n",
    "        if i>0 and i%1000 == 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "            print(\"Saved Model\")\n",
    "    saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x236e2287438>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23694d61a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rMat = np.resize(np.array(rList),[len(rList)//100,100])\n",
    "rMean = np.average(rMat,1)\n",
    "plt.plot(rMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
